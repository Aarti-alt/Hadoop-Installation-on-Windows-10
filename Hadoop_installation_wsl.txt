# HADOOP INSTALLATION IN WSL:

prerequisite - Install ubuntu on WSL (Windows Subsytem For LINUX- a video link is provided for same)

sudo apt update

# Install JAVA
sudo apt install openjdk-8-jdk -y

- check version
java -version; javac -version

-check environment
echo $JAVA_HOME

-set environment if not set
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

#Set Up a Non-Root User for Hadoop Environment

- Install OpenSSH on Ubuntu
sudo apt install openssh-server openssh-client -y

-Create Hadoop User
sudo adduser hdoop
set password or you can leave it blank

-login to hadoopuser
su hadoopusr(username)
type password if any

#Enable Passwordless SSH for Hadoop User

-Generate an SSH key pair and define the location is is to be stored in:
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

-Use the cat command to store the public key as authorized_keys in the ssh directory:
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

-Set the permissions for your user with the chmod command:
chmod 0600 ~/.ssh/authorized_keys

-Verify everything is set up correctly by using the hdoop user to SSH to localhost:
ssh localhost

# if you face error in ssh please view error handlig file. 

#Download and Install Hadoop on Ubuntu

-Visit the official Apache Hadoop project page, and select the version of Hadoop you want to implement.

-A list of Hadoopversions available for download.

-Select your preferred option, and you are presented with a mirror link that allows you to download the Hadoop tar package.

-The download page provides the direct download miror link for Hadoop.

-Use the provided mirror link and download the Hadoop package with the wget command:
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz

-Once the download is complete, extract the files to initiate the Hadoop installation:
tar xzf hadoop-3.2.1.tar.gz (specify the file name that you have downloaded)

(The Hadoop binary files are now located within the hadoop-3.2.1 directory)

-Now lets move file to usr/local 
sudo mv hadoop-3.2.1(specify your directory) /usr/local/hadoop

-Now give folder ownership to hadoop user
sudo chown -R hadoopusr /usr/local

#Single Node Hadoop Deployment (Pseudo-Distributed Mode)

This setup, also called pseudo-distributed mode, allows each Hadoop daemon to run as a single Java process. A Hadoop environment is configured by editing a set of configuration files:

bashrc
hadoop-env.sh
core-site.xml
hdfs-site.xml
mapred-site-xml
yarn-site.xml

-Configure Hadoop Environment Variables (bashrc)
sudo nano ~/.bashrc

Define the Hadoop environment variables by adding the following content to the end of the file:

#Hadoop Related Options
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

Once you add the variables, save and exit the .bashrc file.(use ctrl+s to save and ctrl+x to exit)

It is vital to apply the changes to the current running environment by using the following command:
source ~/.bashrc

# lets configure first file -

1. Edit hadoop-env.sh File
The hadoop-env.sh file serves as a master file to configure YARN, HDFS, MapReduce, and Hadoop-related project settings.
When setting up a single node Hadoop cluster, you need to define which Java implementation is to be utilized. 
Use the previously created $HADOOP_HOME variable to access the hadoop-env.sh file:

sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

-Uncomment the $JAVA_HOME variable (i.e., remove the # sign) and add the full path to the OpenJDK installation on your system. 
 If you have installed the same version as presented in the first part, add the following line-

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 

(The path needs to match the location of the Java installation on your system)

If you need help to locate the correct Java path, run the following command in your terminal window:
which java

The resulting output provides the path to the Java binary directory.

Use the provided path to find the OpenJDK directory with the following command:
readlink -f /usr/bin/java

The section of the path just before the /bin/javac directory needs to be assigned to the $JAVA_HOME variable.

2. Edit core-site.xml File 

The core-site.xml file defines HDFS and Hadoop core properties.

To set up Hadoop in a pseudo-distributed mode, you need to specify the URL for your NameNode, 
and the temporary directory Hadoop uses for the map and reduce process.

Open the core-site.xml file in a text editor:
sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml

Add the following configuration to override the default values for the temporary directory and
add your HDFS URL to replace the default local file system setting:
(make sure you type your location in place of /usr/local/hadoop_tmp - we need to create tmpdata in /usr/local )
(if you have followed this tutorial then you can specify that I have specified)

<configuration>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/usr/local/hadoop_tmp</value>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:9000</value>
</property>
</configuration>

- Again do not forget to create a Linux directory in the location you specified for your temporary data. Use below command
sudo mkdir -p /usr/local/hadoop_tmp

-Change ownership
sudo chown -R hadoopusr /usr/local/hadoop_tmp

3. Edit hdfs-site.xml File

The properties in the hdfs-site.xml file govern the location for storing node metadata, fsimage file, and edit log file. Configure the file by defining the NameNode and DataNode storage directories.

Additionally, the default dfs.replication value of 3 needs to be changed to 1 to match the single node setup.

-Use the following command to open the hdfs-site.xml file for editing:
sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml

-Add the following configuration to the file and, 
 Don't forget to add the NameNode and DataNode directories to your custom locations:

<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/usr/local/hadoop_space/dfsdata/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/usr/local/hadoop_space/dfsdata/datanode</value>
</property>
</configuration>

# Now lets create specified directories
sudo mkdir -p /usr/local/hadoop_space
sudo mkdir -p /usr/local/hadoop_space/dfsdata/namenode
sudo mkdir -p /usr/local/hadoop_space/dfsdata/datanode

-Change ownership
sudo chown -R hadoopusr /usr/local/hadoop_space

4. Edit mapred-site.xml File

Use the following command to access the mapred-site.xml file and define MapReduce values:
sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml

- Add the following configuration to change the default MapReduce framework name value to yarn:
<configuration> 
<property> 
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property> 
</configuration>

5. edit yarn-site.xml File

The yarn-site.xml file is used to define settings relevant to YARN. 
It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.

Open the yarn-site.xml file in a text editor:
sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml

Append the following configuration to the file:

<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</configuration>

Now with this you are done with the basic hadoop installation

# To Start Hadoop

1. Format HDFS NameNode - It is important to format the NameNode before starting Hadoop services for the first time.
$HADOOP_HOME/bin/hdfs namenode -format

The shutdown notification signifies the end of the NameNode format process.

2. Start Hadoop Cluster
$HADOOP_HOME/sbin/start -dfs.sh

This starts namenode, datanode and secondary namenode.

3. Start YARN
$HADOOP_HOME/sbin/start -yarn.sh

This starts resourcemanager and nodemanager.

4. Type this simple command to check if all the daemons are active and running as Java processes:
jps

If everything is working as intended, the resulting list of running Java processes contains all the HDFS and YARN daemons.

#Access Hadoop UI from Browser

-Use your preferred browser and navigate to your localhost URL or IP. 
-the default port number 9870 gives you access to the Hadoop NameNode UI:

http://localhost:9870

-The default port 9864 is used to access individual DataNodes directly from your browser:
http://localhost:9864

-The YARN Resource Manager is accessible on port 8088:
http://localhost:8088


Conclusion
You have successfully installed Hadoop on Ubuntu and deployed it in a pseudo-distributed mode.

# when you start hadoop on your system.

- First 
-restart ssh:
sudo service ssh restart

-Second
-Verify everything is set up correctly by using the hdoop user to SSH to localhost:
ssh localhost




